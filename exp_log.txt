10/18
Carla1 on aicps server running csac_agent_AA w/ 200 penalty -- gpu 0
Carla2 on aicps server running csac_agent_A w/ 100 penalty --gpu 1

Shield always hits the curb after the last object -- options include reducing distance by 1.5 -- removing curbs -- decreasing objects -- ask about the angle

10/18 -- models from server
- casc_model_C is the one with 55-65 km/hr speed 	carla7 (each was taking two gpus)
- casc_model_B is the one with 45-55 km/hr speed 	carla5
- casc_model_A is the one with 35-45 km/hr speed 	carla3
 
 10/18 -- models from our server
 - casc_model_AA is the same as A with penalty 500
 - casc_model_CC is the same as B with penalty 500

__________________________________________________

10/14 -- 1:00 am

- I changed the weight of the speed componenet in the reward to be quivalent to that of the centering.
- training 3 agents on the rcpslserver, all on medium route, 4 objcets, 2500 episodes
1st: 35-45 km/h, 2nd: 45-55km/hr, 3rd: 55-65 km/hr
- concerned about how long it takes to finish an episode (medium route takes 400+ seconds and that's with hitting the curb!)

-----------------------------------------------------

10/13 -- 15:10 pm

- have the first set of offloading results (baselines)
- the retraining of agents 2 and 1 to achieve better results is not working
- agent1 trained-badly again
- Will try to train it on the local machine one last time
- simultaneously can try running casc_agent3 and casc_agent1 on the server w/ 2 obstacles

-------------------------------------------------------

10/11 -- 10:30 am

- casc_agent 3: good results
- server (GPU1): retraining casc_agent1 for better results
- casc_agent2: it learnt a policy to stay idle. retraining on server (GPU0)
- I also started running the scenarios for the offloading scenarios @scale 20Mbps, 35 episodes and worst function analysis (for gaussian and non-gaussian). If i want to run for more episodes

-------------------------------------------------------
10/10 -- 9:30 pm

- retrained casc_agent3 and currently evaluating for 30 episodes (good w/ shield)
- retraining casc_agent2 on the server GPU1 
- trying to retrain casc_agent1 to improve its performance on the not shield scenarios. getting segmenetation errors (this is due to user being assigned illegal memory access for some reason)

---------------------------------------------------------
10/09 -- 1:15

1st set of experiments for casc_agent1 in test_script.sh - success. Now retraining to repeat for the two other agents.

Script used to retrain the agents after random respawning
python run.py --model_name casc_agent2 -obstacle --len_obs 4 --offload_policy local --deadline 100 --phi_scale 10 --srate 900 --obs_start_idx 20 --num_episodes 150 -penalize_dist_obstacle --len_route short --spawn_random --reward reward_speed_centering_angle_add

----------------------------------------------------------
Important (Conclusion:) Testing using the 1484 casc_agent1 controller om medium route, len_5, obs_start_20, has led to good results for safety filter ungaussian, but still has errors gaussian.
----------------------------------------------------------
10/05 -- 22:25 (Conclusion -- all performed really bad, worse than the original, they learnt to go the extreme right and became super aggressive on speed)

Local (multiplicative reward)

Local (multiplicative reward)
python run.py --model_name casc_agent1 -obstacle --img_resolution 80p --arch ResNet152 --offload_policy adaptive --offload_position direct --num_episodes 0 --deadline 100 --len_obs 5 --len_route medium --observation_res 80 -penalize_dist_obstacle

Instance1 - casc-agent2 (RCPS):  (docker container carla5)
Model1: no penalize distance obstacle  

Instance2 - casc_agent3 (RCPS):  (docker container carla6)
Model3: penalize distance obstacle

Instance3 (RCPS):  (No memory available!)
Model3: no penalize distance obstacle

----------------------------------------------------------
08/26 -- 16:30 -- I need to run scripts 2-4

Local (multiplicative reward)
python run.py --model_name casc_agent1 -obstacle --img_resolution 80p --arch ResNet152 --offload_policy adaptive --offload_position direct --num_episodes 0 --deadline 100 --len_obs 4 --len_route medium --observation_res 80 -penalize_dist_obstacle

On ResilientCPS remote server

Training Docker 1 (port 2000) gpu0 -- (multiplicative (5 obs))
python run.py --model_name casc_agent1 -obstacle --img_resolution 80p --arch ResNet152 --offload_policy adaptive --offload_position direct --num_episodes 6000 --deadline 100 --len_obs 5 --len_route medium --observation_res 80 -penalize_dist_obstacle --no_rendering -display_off

Training Docker 2 (port 3000) gpu1 -- (multiplicative (no penalize_dist_obstacle))
python run.py --model_name casc_agent2 -obstacle --img_resolution 80p --arch ResNet152 --offload_policy adaptive --offload_position direct --num_episodes 6000 --deadline 100 --len_obs 5 --len_route medium --observation_res 80 --no_rendering -display_off --port 3000

Training Docker 3 (port 4000) gpu3 -- (additive reward (penalize))
python run.py --model_name casc_agent3 -obstacle --img_resolution 80p --arch ResNet152 --offload_policy adaptive --offload_position direct --num_episodes 6000 --deadline 100 --len_obs 4 --len_route medium --observation_res 80 -penalize_dist_obstacle --no_rendering -display_off
--reward_fn reward_speed_centering_angle_add --port 4000

Training Docker 4 (port 5000) gpu4 -- (multiplicative reward (penalize and long) 5 obs)
python run.py --model_name casc_agent4 -obstacle --img_resolution 80p --arch ResNet152 --offload_policy adaptive --offload_position direct --num_episodes 6000 --deadline 100 --len_obs 5 --len_route long --observation_res 80 -penalize_dist_obstacle --no_rendering -display_off --port 5000
----------------------------------------------------------
Training the casc_agent2 on medium route with 4 obstacles for infinite number of episodes. Using the additive reward. distance_to_obstacle included in the reward.
Something is wrong with the objects spawning
----------------------------------------------------------
08/25 -- 9:52 am
Training the casc_agent1 on medium route with 4 obstacles for infinite number of episodes. Using the multiplicative reward. No distance_to_obstacle or steering angle_difference in the reward (30 episodes).

